{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": null,
>>>>>>> dd20ea2170bc4aab758f0c9359774fc0664a4acf
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> dd20ea2170bc4aab758f0c9359774fc0664a4acf
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings is a dataframe string 1.9 million word vectors length 300\n",
    "with pd.HDFStore('datascience.h5') as hdf:\n",
    "    embeddings = pd.read_hdf(hdf, key='embeddings')\n",
    "    pos_words = pd.read_hdf(hdf, key='pos_words')\n",
    "    neg_words = pd.read_hdf(hdf, key='neg_words')"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.18378</td>\n",
       "      <td>-0.121230</td>\n",
       "      <td>-0.119870</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>-0.191210</td>\n",
       "      <td>-0.066074</td>\n",
       "      <td>-2.9876</td>\n",
       "      <td>0.807950</td>\n",
       "      <td>0.067338</td>\n",
       "      <td>-0.131840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136150</td>\n",
       "      <td>0.109990</td>\n",
       "      <td>-0.334740</td>\n",
       "      <td>-0.046109</td>\n",
       "      <td>0.10780</td>\n",
       "      <td>-0.035657</td>\n",
       "      <td>-0.012921</td>\n",
       "      <td>-0.039038</td>\n",
       "      <td>0.182740</td>\n",
       "      <td>0.146540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.20838</td>\n",
       "      <td>-0.149320</td>\n",
       "      <td>-0.017528</td>\n",
       "      <td>-0.028432</td>\n",
       "      <td>-0.060104</td>\n",
       "      <td>-0.264600</td>\n",
       "      <td>-4.1445</td>\n",
       "      <td>0.629320</td>\n",
       "      <td>0.336720</td>\n",
       "      <td>-0.433950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041244</td>\n",
       "      <td>-0.461820</td>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.546570</td>\n",
       "      <td>-0.25894</td>\n",
       "      <td>0.395150</td>\n",
       "      <td>0.261440</td>\n",
       "      <td>-0.540660</td>\n",
       "      <td>0.211990</td>\n",
       "      <td>-0.009436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.10876</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.222130</td>\n",
       "      <td>-0.121020</td>\n",
       "      <td>-0.048959</td>\n",
       "      <td>0.018135</td>\n",
       "      <td>-3.8174</td>\n",
       "      <td>-0.032631</td>\n",
       "      <td>-0.625940</td>\n",
       "      <td>-0.518980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063131</td>\n",
       "      <td>-0.212300</td>\n",
       "      <td>-0.300880</td>\n",
       "      <td>-0.451610</td>\n",
       "      <td>0.26480</td>\n",
       "      <td>0.075971</td>\n",
       "      <td>-0.406880</td>\n",
       "      <td>-0.296960</td>\n",
       "      <td>0.159390</td>\n",
       "      <td>-0.149020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.09611</td>\n",
       "      <td>-0.257880</td>\n",
       "      <td>-0.358600</td>\n",
       "      <td>-0.328870</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>-0.517740</td>\n",
       "      <td>-4.1582</td>\n",
       "      <td>-0.113710</td>\n",
       "      <td>-0.108480</td>\n",
       "      <td>-0.488850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477810</td>\n",
       "      <td>-0.021213</td>\n",
       "      <td>-0.212360</td>\n",
       "      <td>0.423740</td>\n",
       "      <td>0.14083</td>\n",
       "      <td>0.067498</td>\n",
       "      <td>-0.126750</td>\n",
       "      <td>-0.370300</td>\n",
       "      <td>-0.092774</td>\n",
       "      <td>0.390580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.24837</td>\n",
       "      <td>-0.454610</td>\n",
       "      <td>0.039227</td>\n",
       "      <td>-0.284220</td>\n",
       "      <td>-0.031852</td>\n",
       "      <td>0.263550</td>\n",
       "      <td>-4.6323</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>-0.539280</td>\n",
       "      <td>-0.084454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082736</td>\n",
       "      <td>-0.624690</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.606730</td>\n",
       "      <td>-0.12458</td>\n",
       "      <td>-0.154430</td>\n",
       "      <td>-0.163390</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.154580</td>\n",
       "      <td>-0.380530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5       6    \\\n",
       ",    0.18378 -0.121230 -0.119870  0.015227 -0.191210 -0.066074 -2.9876   \n",
       "the -0.20838 -0.149320 -0.017528 -0.028432 -0.060104 -0.264600 -4.1445   \n",
       ".    0.10876  0.002244  0.222130 -0.121020 -0.048959  0.018135 -3.8174   \n",
       "and -0.09611 -0.257880 -0.358600 -0.328870  0.579500 -0.517740 -4.1582   \n",
       "to  -0.24837 -0.454610  0.039227 -0.284220 -0.031852  0.263550 -4.6323   \n",
       "\n",
       "          7         8         9    ...       290       291       292  \\\n",
       ",    0.807950  0.067338 -0.131840  ...  0.136150  0.109990 -0.334740   \n",
       "the  0.629320  0.336720 -0.433950  ... -0.041244 -0.461820  0.027903   \n",
       ".   -0.032631 -0.625940 -0.518980  ...  0.063131 -0.212300 -0.300880   \n",
       "and -0.113710 -0.108480 -0.488850  ...  0.477810 -0.021213 -0.212360   \n",
       "to   0.013890 -0.539280 -0.084454  ...  0.082736 -0.624690  0.044267   \n",
       "\n",
       "          293      294       295       296       297       298       299  \n",
       ",   -0.046109  0.10780 -0.035657 -0.012921 -0.039038  0.182740  0.146540  \n",
       "the  0.546570 -0.25894  0.395150  0.261440 -0.540660  0.211990 -0.009436  \n",
       ".   -0.451610  0.26480  0.075971 -0.406880 -0.296960  0.159390 -0.149020  \n",
       "and  0.423740  0.14083  0.067498 -0.126750 -0.370300 -0.092774  0.390580  \n",
       "to   0.606730 -0.12458 -0.154430 -0.163390  0.053097  0.154580 -0.380530  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.head()"
   ]
  },
  {
=======
>>>>>>> dd20ea2170bc4aab758f0c9359774fc0664a4acf
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with senticnet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentic_data = pd.read_csv('senticnet5.txt',sep='\\t', keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = []\n",
    "targets_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RepresentsFloat(s):\n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "for index, row in sentic_data.iterrows():\n",
    "    if '_' not in row['CONCEPT']:\n",
    "        words2.append(row['CONCEPT'])\n",
    "        for col in row:\n",
    "            if RepresentsInt(col):\n",
    "                targets_dict[row['CONCEPT']] = float(col)\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors2 = embeddings.reindex(words2).dropna()\n",
    "labels2 = vectors2.index\n",
    "targets2 = []\n",
    "for label in labels2:\n",
    "    targets2.append(targets_dict[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors2, test_vectors2, train_targets2, test_targets2, train_labels2, test_labels2 = \\\n",
    "    train_test_split(vectors2, targets2, labels2, test_size=0.2, random_state=0)\n",
    "\n",
    "train_labels2 = list(train_labels2)\n",
    "test_labels2 = list(test_labels2)\n",
    "model2 = LinearRegression()\n",
    "model2.fit(train_vectors2, train_targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent(word):\n",
    "    vec = embeddings.loc[word].dropna()\n",
    "    return model2.predict([vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with UIC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word = 'horrible'\n",
    "vec = embeddings.loc[word].dropna()\n",
    "model2.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectors = embeddings.reindex(pos_words).dropna()\n",
    "neg_vectors = embeddings.reindex(neg_words).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = LogisticRegression()\n",
    "model.fit(train_vectors, train_targets)\n",
    "accuracy_score(model.predict(test_vectors), test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model.predict(test_vectors2), test_targets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model2.predict(test_vectors), test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
